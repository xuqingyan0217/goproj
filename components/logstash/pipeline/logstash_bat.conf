# 日志的输入源
input {
    beats {
        port => 5044
    }
    # file表示日志是从文件中读取
#     file {
#         # 表示输入的日志文件类型，可以用于在filter中进行条件判断或分类处理
#         # 该类型是我们对于获取到的日志自定义的
#         type => "mysql-slow"
#         path => "/var/logs/mysql-slow.log"
#         # 表示logstash开始读取日志文件的位置，可以选择为beginning（从头开始）或end（从末尾开始）
#         # ogstash从什么位置开始读取文件数据，默认是结束位置，也就是说logstash进程会类似tail-F的形式运行，如果你是要导入原有数据
#         # 要把这个设定改为beginning，logstash进程就从头开始读取，类似less +F的形式
#         # start_position => "beginning"
#         # 表示对输入的日志文件内容进行多行处理，即将多行日志合并为单一的事件进行处理
#         codec => multiline {
#             # 以"# Time:" 为分隔符，中间的所有多行内容归为一行并填充到Event事件中
#             pattern => "^# Time:"
#             # 表示是否对指定的pattern进行否定匹配，即如果为true，则表示匹配不到pattern的行将被合并到上一个事件中，如果为false
#             # 则表示只有匹配到pattern的行才会被合并到上一个事件中。
#             negate => true
#             # 表示合并多行日志时，是将匹配的行合并到上一个事件的头部（previous）还是尾部（next）
#             what => "previous"
#         }
#     }
#     # 收集ApiSix中的请求日志信息
#     file {
#         type => "apisix"
#         path => "/var/logs/apisix/access.log"
#         start_position => "beginning"
#     }
}

filter {
    if [type] == "mysql-slow" {
        # grok从数据源中匹配需要的数据
           grok {
               match => {
                   # 对"message"字段进行了正则表达式匹配，解析了日志中的各个字段，如user，clientip，row_id，Query_time, lock_time
                   # Row_sent, Rows_examined, database, action等，该正则表达式的意思是根据给定的日志格式，从"message"字段中提取出
                   # 对应的字段值，并存储到对应的字段中，这样可以将多行日志合并为单一的事件，并提取出需要的字段进行后续处理
                   "message" => "(?m)^# Time:.*\s+#\s+User@Host:\s+%{USER:user}\[[^\]]+\]\s+@\s+(?:(?<clientip>\s*)
                   )?\[(?:%{IPV4:clientip})?\]\s+Id:\s+%{NUMBER:row_id:int}\n#\s+Query_time:\s+%{NUMBER:Query_time
                   :float}\s+Lock_time:\s+%{NUMBER:lock_time:float}\s+Rows_sent:\s+%{NUMBER:Row_sent:int}\s
                   +Rows_examined:\s+%{NUMBER:Rows_examined:int}\n\s*(?:use %{DATA:database};\s*\n)
                   ?SET\s+timestamp=%{NUMBER:timestamp};\n\s*(?<sql>(?<action>\w+)\b.*)$"
               }
           }
    }

    if [type] == "apisix" {
        grok {
            match => {
            #    "message" => "%{IPORHOST:remote_addr} - - \[%{HTTPDATE:time_local}\] %{HOSTPORT:listen_on}
            #    \"%{WORD:method} %{URIPATHPARAM:uri} (?<httpversion>(?<action>\w+\/\S+))\" %{NUMBER:status}
            #    %{NUMBER:body_bytes_sent} \d+.\d+ \"-\" \"%{GREEDYDATA:http_referer}\" %{HOSTPORT:proxy}
            #    %{NUMBER:proxy_status} %{NUMBER:proxy_time} \"http://10.0.0.17:9080"
                "message" => "%{IPORHOST:remote_addr} - - \[%{HTTPDATE:time_local}\] %{HOSTPORT:listen_on}
                \"%{WORD:method} %{URIPATHPARAM:uri} (?<httpversion>(?<action>\w+\/\S+))\" %{NUMBER:status}
                %{NUMBER:body_bytes_sent} \d+.\d+ \"-\" \"%{GREEDYDATA:http_referer}\" %{HOSTPORT:proxy}
                %{NUMBER:proxy_status} %{NUMBER:proxy_time} \"http://10.0.0.17:9080\""
            }
        }
    }

    # 作用是将日志事件中的timestamp字段解析为日期时间格式，并将其作为事件的事件戳
    date {
        # 表示要匹配的字段和日期时间格式，其中"timestamp"为要解析的字段，"UNIX"表示日期时间格式为UNIX时间戳
        match => [ "timestamp", "UNIX" ]
        # 表示在处理完日期时间格式后，将原始的"timestamp"字段移除
        remove_field => [ "timestamp" ]
    }
}

output {
    if [type] == "mysql-slow" {
        # 将日志输出到Elasticsearch
        elasticsearch {
            hosts => ["elasticsearch:9200"]
            index => "mysql_slow_logs"
        }
    }
    if [type] == "apisix" {
        elasticsearch {
            hosts => ["elasticsearch:9200"]
            index => "beats_apisix_access_logs"
        }
    }

    # 标准输出到控制台
    stdout {
        codec => rubydebug
    }
}